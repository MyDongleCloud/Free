{
	"module": "llamacpp",
	"name": "llama.cpp",
	"title": "LLM Inference",
	"category": "Developer",
	"version": "b7315",
	"installMethod": "clone",
	"web": true,
	"finished": true,
	"ai": true,
	"database": "",
	"github": "ggml-org/llama.cpp",
	"description": "A tool to enable LLM inference with minial setup and outstanding performance.",
	"keywords": [
		"LLM",
		"Local",
		"AI",
		"Language"
	],
	"proprietary": "",
	"default": {
		"enabled": true,
		"localPort": 9442,
		"permissions": [
			"_groupadmin_"
		],
		"reverseProxy": [
			{
				"type": "http",
				"port": 8099,
				"path": "/"
			},
			{
				"type": "ws",
				"port": 8099,
				"path": "/"
			}
		],
		"services": [
			"llamacpp"
		],
		"setup": true,
		"setupDependencies": []
	}
}
